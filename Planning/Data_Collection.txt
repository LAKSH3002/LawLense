Phase 1 of the project: Data Collection

Researching about constitution, laws, articles, cases, judgements
Collecting data from multiple platforms (web scraping, APIs, Databases)
The major platform from where data has been extracted is Indian Kanoon.

Detailed Process

A - The pages were inspected, all essential html tags were noted down
B - The base URL with variable parameters were defined along with two lists where data would be stored
C - A for loop was run where proper link was generated and then data and content were extracted from the link
D - The data was stored under 2 different lists, converted to dataframe and then merged
E - After merging the final dataset of the constitution articles were created.
F - Next was to extract cases links from the Indian Kanoon website and create another dataset
G - So for a particular query, whatever the top results were there, they were collected
H - Queires include right to speech, some specific cases and such topics
I - All query, case title and case URLs were collected


There is one dataset used in this Project

Dataset description
Name: Articles_with_parts.csv
3 columns: URL, title, and content
This dataset is used and embedded in Gemini AI API model to give them context
The model reads the embedded values and generates answers to the Queires
Queries must be related to the articles and parts of the constitution of India.

NOTE: There is one more dataset in the project

Dataset description
Name: case results basics
3 columns: query, case title, case URL
So there was one more column of content, but had to eliminate it because there were too many empty values.
Like out of 1000s there were about 600-700 empty values, the extraction of data was not taking place properly
Because the html tags differ in a lot of case pages
So then I had to plan to create a case summarizer, rather than creating a chatbot.
So this dataset is never used in this project, because of empty values.

So a new set of code is writtent to summarize case based on the URL of the case given.